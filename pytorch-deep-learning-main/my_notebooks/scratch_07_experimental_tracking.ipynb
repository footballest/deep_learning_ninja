{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aae1c71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "0.23.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55f2786e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sauravkrishna/Documents/Python_stuffs/projects/deep_learning_ninja/pytorch-deep-learning-main/my_notebooks'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba688095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'pytorch-deep-learning-main/my_notebooks/'\n",
      "/Users/sauravkrishna/Documents/Python_stuffs/projects/deep_learning_ninja/pytorch-deep-learning-main/my_notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd pytorch-deep-learning-main/my_notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "379d6a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sauravkrishna/Documents/Python_stuffs/projects/deep_learning_ninja/pytorch-deep-learning-main/my_notebooks'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ac000ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine, data_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95b35955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b320d",
   "metadata": {},
   "source": [
    "## 1. get data\n",
    "\n",
    "- download the pizza, sushi, steak data \n",
    "- turn into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c85d4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8179601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pizza_steak_sushi_20_percent.zip'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\").name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9122af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "def download_data(source: str,\n",
    "                  destination: str,\n",
    "                  remove_source: bool=True) -> Path:\n",
    "    \"\"\"Downloads data from a source URL and unzips it to a destination.\"\"\"\n",
    "    # setup paths to data folder\n",
    "    data_path = Path(\"data/\")\n",
    "    image_path = data_path / destination\n",
    "\n",
    "    if image_path.is_dir():\n",
    "        print(f\"[INFO] {image_path} directory already exists, skipping download.\")\n",
    "    else:\n",
    "        print(f\"[INFO] {image_path} directory does not exist, creating one...\")\n",
    "        image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # download the target data \n",
    "    target_file = Path(source).name\n",
    "    with open(data_path / target_file, \"wb\") as f:\n",
    "        request = requests.get(source)\n",
    "        print(f\"[INFO] Downloading target file from {source}....\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
    "        print(f\"[INFO] Unzipping {target_file} data\")\n",
    "        zip_ref.extractall(image_path)\n",
    "    \n",
    "    if remove_source:\n",
    "        os.remove(data_path / target_file)\n",
    "    \n",
    "    return image_path\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07be8639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory already exists, skipping download.\n",
      "[INFO] Downloading target file from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip....\n",
      "[INFO] Unzipping pizza_steak_sushi_20_percent.zip data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data/pizza_steak_sushi')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\"\n",
    "image_path = download_data(\n",
    "    url,\n",
    "    destination=\"pizza_steak_sushi\"\n",
    ")\n",
    "image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc6cbe",
   "metadata": {},
   "source": [
    "## 2. Create Datasets and Dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c72c9",
   "metadata": {},
   "source": [
    "### 2.1 Create DataLoaders with manual transforms\n",
    "\n",
    "the goal with transforms is to ensure your custom data is formatted in a reproducible way as well as way that will suit the pretrained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c211d705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/pizza_steak_sushi/train'),\n",
       " PosixPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup the directiories\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a012289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "988ff77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "std=[0.229, 0.224, 0.225])\n",
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "109d0988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manually created transforms: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x107b74e80>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1084aa370>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create transform pipeline manualluy\n",
    "from torchvision import transforms\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "print(f\"manually created transforms: {manual_transforms}\")\n",
    "\n",
    "# create dataloaders\n",
    "from going_modular import data_setup\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms,\n",
    "    batch_size=32,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e6c58",
   "metadata": {},
   "source": [
    "### 2.1 Create DataLoaders with auto transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8170c8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet_B0_Weights.IMAGENET1K_V1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3032a22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_transforms = weights.transforms()\n",
    "auto_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8663189b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1084aa3a0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1084aab80>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=auto_transforms,\n",
    "    batch_size=32,\n",
    "    num_workers=0   \n",
    ")\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8424b",
   "metadata": {},
   "source": [
    "## 3. Getting a pretrained model, freeze the base layers and change the classifier head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a75320d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# older medthod\n",
    "#model = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# get the weights first\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "# pass the weights to the model\n",
    "model = torchvision.models.efficientnet_b0(weights=weights)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73b6a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    #print(param)\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1033a550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=True)\n",
       "  (1): Linear(in_features=1280, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## adjust the classifier head\n",
    "from torch import nn\n",
    "set_seeds()\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=len(class_names))\n",
    ")\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c815e57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n",
       "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n",
       "============================================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (G): 12.31\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model,\n",
    "input_size=(32, 3, 224, 224),\n",
    "verbose=0,\n",
    "col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "col_width=20,\n",
    "row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f35bc",
   "metadata": {},
   "source": [
    "## 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24e8ea52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CrossEntropyLoss(),\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     decoupled_weight_decay: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## define loss and optimizer\n",
    "from torch import nn, optim\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "lr=0.001)\n",
    "loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38915630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (3.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (1.23.5)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (11.3.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (6.33.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (80.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from tensorboard) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.23.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/anaconda3/envs/d2l/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e61823b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x10839ee80>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=\"runs/efficientnet_experiment_1\")\n",
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40ca4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.engine import train_step, test_step\n",
    "from typing import Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          writer: SummaryWriter,\n",
    "          device: torch.device) -> Dict[str, List[float]]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "              train_acc: [...],\n",
    "              test_loss: [...],\n",
    "              test_acc: [...]} \n",
    "    For example if training for epochs=2: \n",
    "             {train_loss: [2.0616, 1.0537],\n",
    "              train_acc: [0.3945, 0.3945],\n",
    "              test_loss: [1.2641, 1.5706],\n",
    "              test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        # new: experimental tracking with tensorboard\n",
    "        writer.add_scalars(\n",
    "            main_tag=\"Loss\",\n",
    "            tag_scalar_dict={\n",
    "                \"train_loss\": train_loss,\n",
    "                \"test_loss\": test_loss\n",
    "            },\n",
    "            global_step=epoch\n",
    "        )\n",
    "        writer.add_scalars(\n",
    "            main_tag=\"Accuracy\",\n",
    "            tag_scalar_dict={\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc\n",
    "            },\n",
    "            global_step=epoch\n",
    "        )\n",
    "        writer.add_graph(\n",
    "            model=model,\n",
    "            input_to_model=torch.randn(32, 3, 224, 224).to(device)\n",
    "        )\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab11bd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "611c3eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e86bf60b624c34862121143ddd0ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9363 | train_acc: 0.6042 | test_loss: 0.6657 | test_acc: 0.8926\n",
      "Epoch: 2 | train_loss: 0.6483 | train_acc: 0.8604 | test_loss: 0.5250 | test_acc: 0.8943\n",
      "Epoch: 3 | train_loss: 0.5283 | train_acc: 0.8583 | test_loss: 0.4478 | test_acc: 0.8926\n",
      "Epoch: 4 | train_loss: 0.4383 | train_acc: 0.8625 | test_loss: 0.3900 | test_acc: 0.9152\n",
      "Epoch: 5 | train_loss: 0.3870 | train_acc: 0.8917 | test_loss: 0.3661 | test_acc: 0.8971\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "set_seeds()\n",
    "results = train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    epochs=5,\n",
    "    writer=writer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f7055c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 10412), started 0:00:21 ago. (Use '!kill 10412' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2e0fe0e71b2dbad2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2e0fe0e71b2dbad2\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98088907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee815a89",
   "metadata": {},
   "source": [
    "## 6. Create a function to prepare a `SummaryWriter` instance\n",
    "\n",
    "By default a summary writer saves to the `runs/` directory. \n",
    "what if we want to save different experiments to different folders?\n",
    "\n",
    "for examples we'd like to track:\n",
    "    * experiment date/timestamp\n",
    "    * experiment name\n",
    "    * model name\n",
    "    * experiment hyperparameters\n",
    "\n",
    "Create  function to create a `SummaryWriter()` instance to take all of these things into account.\n",
    "\n",
    "So ideally we end up tracking experiments to a dir:\n",
    "`runs/YYYY-MM-DD/experiment_name/model_name/extra`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4e8ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_writer(experiment_name: str,\n",
    "model_name: str,\n",
    "extra: str=None) -> SummaryWriter:\n",
    "    \"\"\"Creates a torch.utils.tensorboard.SummaryWriter instance tracking to a specific directory\"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # get timestamp of current date in reverse order\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if extra:\n",
    "        # create log direc path\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "    print(f\"[INFO] Creating SummaryWriter at: {log_dir}\")\n",
    "    return SummaryWriter(log_dir=log_dir)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c8f3dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating SummaryWriter at: runs/2026-01-08/data_10_percent/effnetb0/epochs_5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x11835e9d0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_writer = create_writer(\n",
    "    experiment_name=\"data_10_percent\",\n",
    "    model_name=\"effnetb0\",\n",
    "    extra=\"epochs_5\")\n",
    "example_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e1eba",
   "metadata": {},
   "source": [
    "### 6.1 Update the `train()` to include a `writer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e487ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.engine import train_step, test_step\n",
    "from typing import Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          writer: torch.utils.tensorboard.SummaryWriter,\n",
    "          device: torch.device) -> Dict[str, List[float]]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "              train_acc: [...],\n",
    "              test_loss: [...],\n",
    "              test_acc: [...]} \n",
    "    For example if training for epochs=2: \n",
    "             {train_loss: [2.0616, 1.0537],\n",
    "              train_acc: [0.3945, 0.3945],\n",
    "              test_loss: [1.2641, 1.5706],\n",
    "              test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        # new: experimental tracking with tensorboard\n",
    "        if writer:\n",
    "            writer.add_scalars(\n",
    "                main_tag=\"Loss\",\n",
    "                tag_scalar_dict={\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"test_loss\": test_loss\n",
    "                },\n",
    "                global_step=epoch\n",
    "            )\n",
    "            writer.add_scalars(\n",
    "                main_tag=\"Accuracy\",\n",
    "                tag_scalar_dict={\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"test_acc\": test_acc\n",
    "                },\n",
    "                global_step=epoch\n",
    "            )\n",
    "            writer.add_graph(\n",
    "                model=model,\n",
    "                input_to_model=torch.randn(32, 3, 224, 224).to(device)\n",
    "            )\n",
    "\n",
    "            writer.close()\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42631235",
   "metadata": {},
   "source": [
    "## 7. Setting up a series of modeeling experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db14bd",
   "metadata": {},
   "source": [
    "### 7.1  what kind odf expeiments to run:\n",
    "\n",
    "- different models - limitless, but one cant test everything ofc\n",
    "SO what should we be testigng - at minimum atleast?\n",
    "- no. of epochs\n",
    "- no. of hidden layers/units\n",
    "- change the amount of data\n",
    "- different model architectures\n",
    "- different optimizers\n",
    "- differnt hyperparameter setting ons the model and of the learning algorithm\n",
    "\n",
    "This is why transfer learning is so powerful!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd54e9",
   "metadata": {},
   "source": [
    "### 7.2 What are the experiments we are going to test now:\n",
    "- model size, - EffnetB0 v/s EffnetB2\n",
    "- Dataset size - 10% vs 20%\n",
    "- training time - no. of epochs 5 v/s 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63790f0f",
   "metadata": {},
   "source": [
    "### 7.3 Downloading datasets \n",
    "1. \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\"\n",
    "2. \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7877c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory already exists, skipping download.\n",
      "[INFO] Downloading target file from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip....\n",
      "[INFO] Unzipping pizza_steak_sushi.zip data\n",
      "[INFO] data/pizza_steak_sushi_20_percent directory already exists, skipping download.\n",
      "[INFO] Downloading target file from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip....\n",
      "[INFO] Unzipping pizza_steak_sushi_20_percent.zip data\n"
     ]
    }
   ],
   "source": [
    "# Download 10 percent and 20 percent training data (if necessary)\n",
    "data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                                     destination=\"pizza_steak_sushi\")\n",
    "\n",
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "046900bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sauravkrishna/Documents/Python_stuffs/projects/deep_learning_ninja/pytorch-deep-learning-main/my_notebooks'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f62c0b",
   "metadata": {},
   "source": [
    "### 7.4 Transform the datasets and create dataloaders\n",
    "\n",
    "1. Resize the images to (224, 224)\n",
    "2. Totensor\n",
    "3. Normalise so that the inputs have the same data disribution as the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2817693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training directory 10%: data/pizza_steak_sushi/train\n",
      "Training directory 20%: data/pizza_steak_sushi_20_percent/train\n",
      "Testing directory: data/pizza_steak_sushi/test\n"
     ]
    }
   ],
   "source": [
    "# Setup training directory paths\n",
    "train_dir_10_percent = data_10_percent_path / \"train\"\n",
    "train_dir_20_percent = data_20_percent_path / \"train\"\n",
    "\n",
    "# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n",
    "test_dir = data_10_percent_path / \"test\"\n",
    "\n",
    "# Check the directories\n",
    "print(f\"Training directory 10%: {train_dir_10_percent}\")\n",
    "print(f\"Training directory 20%: {train_dir_20_percent}\")\n",
    "print(f\"Testing directory: {test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5729cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Create a transform to normalize data distribution to be inline with ImageNet\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n",
    "                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n",
    "\n",
    "# Compose transforms into a pipeline\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # 1. Resize the images\n",
    "    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 & 1\n",
    "    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63738b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches of size 32 in 10 percent training data: 15\n",
      "Number of batches of size 32 in 20 percent training data: 15\n",
      "Number of batches of size 32 in testing data: 7 (all experiments will use the same test set)\n",
      "Number of classes: 3, class names: ['pizza', 'steak', 'sushi']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create 10% training and test DataLoaders\n",
    "train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n",
    "    test_dir=test_dir, \n",
    "    transform=simple_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Create 20% training and test data DataLoders\n",
    "train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
    "    test_dir=test_dir,\n",
    "    transform=simple_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(test_dataloader)} (all experiments will use the same test set)\")\n",
    "print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6dbb3937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1066727c0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x10666a580>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader_20_percent, test_dataloader_20_percent, class_names_20_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba91b583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader_10_percent), len(train_dataloader_20_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "913ab569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 450)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader_10_percent.dataset), len(train_dataloader_20_percent.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ecc8634e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/pizza_steak_sushi/train'),\n",
       " PosixPath('data/pizza_steak_sushi_20_percent/train'))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader_10_percent.dataset.root, train_dataloader_20_percent.dataset.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0025f181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pizza_steak_sushi/train\n",
      "data/pizza_steak_sushi_20_percent/train\n",
      "data/pizza_steak_sushi/train\n",
      "data/pizza_steak_sushi_20_percent/train\n",
      "480 450\n"
     ]
    }
   ],
   "source": [
    "print(train_dir_10_percent)\n",
    "print(train_dir_20_percent)\n",
    "print(train_dataloader_10_percent.dataset.root)\n",
    "print(train_dataloader_20_percent.dataset.root)\n",
    "print(len(train_dataloader_10_percent.dataset), len(train_dataloader_20_percent.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f2ea1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of in_features to final layer of EfficientNetB2: 1408\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "# 1. Create an instance of EffNetB2 with pretrained weights\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n",
    "\n",
    "# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n",
    "# summary(model=effnetb2, \n",
    "#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# ) \n",
    "\n",
    "# 3. Get the number of in_features of the EfficientNetB2 classifier layer\n",
    "print(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "44d0ea37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1408, 7, 7]     --                   True\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n",
       "│    │    └─MBConv (1)                                       [32, 16, 112, 112]   [32, 16, 112, 112]   612                  True\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n",
       "│    │    └─MBConv (2)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 48, 28, 28]     --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 48, 28, 28]     16,518               True\n",
       "│    │    └─MBConv (1)                                       [32, 48, 28, 28]     [32, 48, 28, 28]     43,308               True\n",
       "│    │    └─MBConv (2)                                       [32, 48, 28, 28]     [32, 48, 28, 28]     43,308               True\n",
       "│    └─Sequential (4)                                        [32, 48, 28, 28]     [32, 88, 14, 14]     --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 48, 28, 28]     [32, 88, 14, 14]     50,300               True\n",
       "│    │    └─MBConv (1)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     123,750              True\n",
       "│    │    └─MBConv (2)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     123,750              True\n",
       "│    │    └─MBConv (3)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     123,750              True\n",
       "│    └─Sequential (5)                                        [32, 88, 14, 14]     [32, 120, 14, 14]    --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 88, 14, 14]     [32, 120, 14, 14]    149,158              True\n",
       "│    │    └─MBConv (1)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    237,870              True\n",
       "│    │    └─MBConv (2)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    237,870              True\n",
       "│    │    └─MBConv (3)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    237,870              True\n",
       "│    └─Sequential (6)                                        [32, 120, 14, 14]    [32, 208, 7, 7]      --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 120, 14, 14]    [32, 208, 7, 7]      301,406              True\n",
       "│    │    └─MBConv (1)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      686,868              True\n",
       "│    │    └─MBConv (2)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      686,868              True\n",
       "│    │    └─MBConv (3)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      686,868              True\n",
       "│    │    └─MBConv (4)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      686,868              True\n",
       "│    └─Sequential (7)                                        [32, 208, 7, 7]      [32, 352, 7, 7]      --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 208, 7, 7]      [32, 352, 7, 7]      846,900              True\n",
       "│    │    └─MBConv (1)                                       [32, 352, 7, 7]      [32, 352, 7, 7]      1,888,920            True\n",
       "│    └─Conv2dNormActivation (8)                              [32, 352, 7, 7]      [32, 1408, 7, 7]     --                   True\n",
       "│    │    └─Conv2d (0)                                       [32, 352, 7, 7]      [32, 1408, 7, 7]     495,616              True\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1408, 7, 7]     [32, 1408, 7, 7]     2,816                True\n",
       "│    │    └─SiLU (2)                                         [32, 1408, 7, 7]     [32, 1408, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1408, 7, 7]     [32, 1408, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1408]           [32, 1000]           --                   True\n",
       "│    └─Dropout (0)                                           [32, 1408]           [32, 1408]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1408]           [32, 1000]           1,409,000            True\n",
       "============================================================================================================================================\n",
       "Total params: 9,109,994\n",
       "Trainable params: 9,109,994\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 21.09\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 5017.79\n",
       "Params size (MB): 36.44\n",
       "Estimated Total Size (MB): 5073.49\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=effnetb2, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a21292b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "# Get num out features (one for each class pizza, steak, sushi)\n",
    "OUT_FEATURES = len(class_names)\n",
    "\n",
    "# Create an EffNetB0 feature extractor\n",
    "def create_effnetb0():\n",
    "    # 1. Get the base model with pretrained weights and send to target device\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "    # 2. Freeze the base model layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 3. Set the seeds\n",
    "    set_seeds()\n",
    "\n",
    "    # 4. Change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "\n",
    "    # 5. Give the model a name\n",
    "    model.name = \"effnetb0\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model\n",
    "\n",
    "# Create an EffNetB2 feature extractor\n",
    "def create_effnetb2():\n",
    "    # 1. Get the base model with pretrained weights and send to target device\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "\n",
    "    # 2. Freeze the base model layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 3. Set the seeds\n",
    "    set_seeds()\n",
    "\n",
    "    # 4. Change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "\n",
    "    # 5. Give the model a name\n",
    "    model.name = \"effnetb2\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e1f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb0 = create_effnetb0() \n",
    "effnetb2 = create_effnetb2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9583121",
   "metadata": {},
   "source": [
    "### 7.6 Create experiments and set up training code\n",
    "\n",
    "We've prepared our data and prepared our models, the time has come to setup some experiments!\n",
    "\n",
    "We'll start by creating two lists and a dictionary:\n",
    "1. A list of the number of epochs we'd like to test (`[5, 10]`)\n",
    "2. A list of the models we'd like to test (`[\"effnetb0\", \"effnetb2\"]`)\n",
    "3. A dictionary of the different training DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8c08e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create epochs list\n",
    "num_epochs = [5, 10]\n",
    "\n",
    "# 2. Create models list (need to create a new model for each experiment)\n",
    "models = [\"effnetb0\", \"effnetb2\"]\n",
    "\n",
    "# 3. Create dataloaders dictionary for various dataloaders\n",
    "train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n",
    "                     \"data_20_percent\": train_dataloader_20_percent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b77781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Experiment number: 1\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] DataLoader: data_10_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created new effnetb0 model.\n",
      "[INFO] Creating SummaryWriter at: runs/2026-01-08/data_10_percent/effnetb0/5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c15a5af3d1b47b9a839169744e4869e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9681 | train_acc: 0.5875 | test_loss: 0.6783 | test_acc: 0.8419\n",
      "Epoch: 2 | train_loss: 0.6719 | train_acc: 0.8479 | test_loss: 0.5097 | test_acc: 0.8957\n",
      "Epoch: 3 | train_loss: 0.5350 | train_acc: 0.8646 | test_loss: 0.4291 | test_acc: 0.9091\n",
      "Epoch: 4 | train_loss: 0.4335 | train_acc: 0.8958 | test_loss: 0.3773 | test_acc: 0.9135\n",
      "Epoch: 5 | train_loss: 0.3823 | train_acc: 0.9000 | test_loss: 0.3434 | test_acc: 0.9180\n",
      "[INFO] Saving model to: models/07_effnetb0_data_10_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 2\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] DataLoader: data_10_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created new effnetb2 model.\n",
      "[INFO] Creating SummaryWriter at: runs/2026-01-08/data_10_percent/effnetb2/5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c752cf713d4430ae19a375c0695a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9845 | train_acc: 0.5375 | test_loss: 0.7641 | test_acc: 0.8642\n",
      "Epoch: 2 | train_loss: 0.7038 | train_acc: 0.8417 | test_loss: 0.6132 | test_acc: 0.9001\n",
      "Epoch: 3 | train_loss: 0.5658 | train_acc: 0.8583 | test_loss: 0.5175 | test_acc: 0.8912\n",
      "Epoch: 4 | train_loss: 0.4506 | train_acc: 0.9146 | test_loss: 0.4670 | test_acc: 0.9046\n",
      "Epoch: 5 | train_loss: 0.4070 | train_acc: 0.9042 | test_loss: 0.4259 | test_acc: 0.9046\n",
      "[INFO] Saving model to: models/07_effnetb2_data_10_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 3\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] DataLoader: data_10_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created new effnetb0 model.\n",
      "[INFO] Creating SummaryWriter at: runs/2026-01-08/data_10_percent/effnetb0/10_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f6269f0d1a41f883961da82ea243cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9681 | train_acc: 0.5875 | test_loss: 0.6783 | test_acc: 0.8419\n",
      "Epoch: 2 | train_loss: 0.6719 | train_acc: 0.8479 | test_loss: 0.5097 | test_acc: 0.8957\n",
      "Epoch: 3 | train_loss: 0.5350 | train_acc: 0.8646 | test_loss: 0.4291 | test_acc: 0.9091\n",
      "Epoch: 4 | train_loss: 0.4335 | train_acc: 0.8958 | test_loss: 0.3773 | test_acc: 0.9135\n",
      "Epoch: 5 | train_loss: 0.3823 | train_acc: 0.9000 | test_loss: 0.3434 | test_acc: 0.9180\n",
      "Epoch: 6 | train_loss: 0.3637 | train_acc: 0.9021 | test_loss: 0.3161 | test_acc: 0.9359\n",
      "Epoch: 7 | train_loss: 0.3366 | train_acc: 0.8958 | test_loss: 0.2930 | test_acc: 0.9269\n",
      "Epoch: 8 | train_loss: 0.2979 | train_acc: 0.9250 | test_loss: 0.2815 | test_acc: 0.9269\n",
      "Epoch: 9 | train_loss: 0.2729 | train_acc: 0.9313 | test_loss: 0.2642 | test_acc: 0.9314\n",
      "Epoch: 10 | train_loss: 0.2795 | train_acc: 0.9208 | test_loss: 0.2520 | test_acc: 0.9269\n",
      "[INFO] Saving model to: models/07_effnetb0_data_10_percent_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 4\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] DataLoader: data_10_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created new effnetb2 model.\n",
      "[INFO] Creating SummaryWriter at: runs/2026-01-08/data_10_percent/effnetb2/10_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cd7b75058648b3b1d29461017860b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9845 | train_acc: 0.5375 | test_loss: 0.7641 | test_acc: 0.8642\n",
      "Epoch: 2 | train_loss: 0.7038 | train_acc: 0.8417 | test_loss: 0.6132 | test_acc: 0.9001\n",
      "Epoch: 3 | train_loss: 0.5658 | train_acc: 0.8583 | test_loss: 0.5175 | test_acc: 0.8912\n",
      "Epoch: 4 | train_loss: 0.4506 | train_acc: 0.9146 | test_loss: 0.4670 | test_acc: 0.9046\n",
      "Epoch: 5 | train_loss: 0.4070 | train_acc: 0.9042 | test_loss: 0.4259 | test_acc: 0.9046\n",
      "Epoch: 6 | train_loss: 0.3649 | train_acc: 0.9271 | test_loss: 0.3961 | test_acc: 0.9135\n",
      "Epoch: 7 | train_loss: 0.3336 | train_acc: 0.9104 | test_loss: 0.3799 | test_acc: 0.9046\n",
      "Epoch: 8 | train_loss: 0.3148 | train_acc: 0.9229 | test_loss: 0.3584 | test_acc: 0.9180\n",
      "Epoch: 9 | train_loss: 0.3063 | train_acc: 0.9146 | test_loss: 0.3365 | test_acc: 0.9314\n",
      "Epoch: 10 | train_loss: 0.2744 | train_acc: 0.9333 | test_loss: 0.3400 | test_acc: 0.8926\n",
      "[INFO] Saving model to: models/07_effnetb2_data_10_percent_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 5\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] DataLoader: data_20_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created new effnetb0 model.\n",
      "[INFO] Creating SummaryWriter at: runs/2026-01-08/data_20_percent/effnetb0/5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dceb838c7c75450783eea0d09fbc6814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9679 | train_acc: 0.5583 | test_loss: 0.6544 | test_acc: 0.8600\n",
      "Epoch: 2 | train_loss: 0.6778 | train_acc: 0.8417 | test_loss: 0.5436 | test_acc: 0.9135\n",
      "Epoch: 3 | train_loss: 0.5493 | train_acc: 0.8750 | test_loss: 0.4431 | test_acc: 0.9001\n",
      "Epoch: 4 | train_loss: 0.4622 | train_acc: 0.8938 | test_loss: 0.3995 | test_acc: 0.9091\n",
      "Epoch: 5 | train_loss: 0.4618 | train_acc: 0.8792 | test_loss: 0.3843 | test_acc: 0.9001\n",
      "[INFO] Saving model to: models/07_effnetb0_data_20_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 6\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] DataLoader: data_20_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created new effnetb2 model.\n",
      "[INFO] Creating SummaryWriter at: runs/2026-01-08/data_20_percent/effnetb2/5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571ac63a3da04a19bcc2ab0ee49d4f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9805 | train_acc: 0.5354 | test_loss: 0.7724 | test_acc: 0.8285\n",
      "Epoch: 2 | train_loss: 0.7483 | train_acc: 0.7917 | test_loss: 0.6445 | test_acc: 0.8703\n",
      "Epoch: 3 | train_loss: 0.5664 | train_acc: 0.8958 | test_loss: 0.5609 | test_acc: 0.9091\n",
      "Epoch: 4 | train_loss: 0.5192 | train_acc: 0.8812 | test_loss: 0.4837 | test_acc: 0.9016\n",
      "Epoch: 5 | train_loss: 0.5245 | train_acc: 0.8479 | test_loss: 0.4661 | test_acc: 0.8882\n",
      "[INFO] Saving model to: models/07_effnetb2_data_20_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 7\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] DataLoader: data_20_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created new effnetb0 model.\n",
      "[INFO] Creating SummaryWriter at: runs/2026-01-08/data_20_percent/effnetb0/10_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f8d91175bc4e6bb96a8dd423783953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9679 | train_acc: 0.5583 | test_loss: 0.6544 | test_acc: 0.8600\n",
      "Epoch: 2 | train_loss: 0.6778 | train_acc: 0.8417 | test_loss: 0.5436 | test_acc: 0.9135\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from going_modular.utils import save_model\n",
    "\n",
    "# 1. Set the random seeds\n",
    "set_seeds(seed=42)\n",
    "\n",
    "# 2. Keep track of experiment numbers\n",
    "experiment_number = 0\n",
    "\n",
    "# 3. Loop through each DataLoader\n",
    "for dataloader_name, train_dataloader in train_dataloaders.items():\n",
    "\n",
    "    # 4. Loop through each number of epochs\n",
    "    for epochs in num_epochs: \n",
    "\n",
    "        # 5. Loop through each model name and create a new model based on the name\n",
    "        for model_name in models:\n",
    "\n",
    "            # 6. Create information print outs\n",
    "            experiment_number += 1\n",
    "            print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "            print(f\"[INFO] Model: {model_name}\")\n",
    "            print(f\"[INFO] DataLoader: {dataloader_name}\")\n",
    "            print(f\"[INFO] Number of epochs: {epochs}\")  \n",
    "\n",
    "            # 7. Select the model\n",
    "            if model_name == \"effnetb0\":\n",
    "                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n",
    "            else:\n",
    "                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n",
    "            \n",
    "            # 8. Create a new loss and optimizer for every model\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "            # 9. Train target model with target dataloaders and track experiments\n",
    "            train(model=model,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  test_dataloader=test_dataloader, \n",
    "                  optimizer=optimizer,\n",
    "                  loss_fn=loss_fn,\n",
    "                  epochs=epochs,\n",
    "                  device=device,\n",
    "                  writer=create_writer(experiment_name=dataloader_name,\n",
    "                                       model_name=model_name,\n",
    "                                       extra=f\"{epochs}_epochs\"))\n",
    "            \n",
    "            # 10. Save the model to file so we can get back the best model\n",
    "            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n",
    "            save_model(model=model,\n",
    "                       target_dir=\"models\",\n",
    "                       model_name=save_filepath)\n",
    "            print(\"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83fd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6462a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a82a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
